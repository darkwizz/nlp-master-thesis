# papuGaPT2 Experiments
Here are presented the model description, revisions' details and the results

## The model
[Original GPT paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
[Original GPT2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

`papuGaPT2` is trained on the Polish subset of [OSCAR](https://aclanthology.org/2020.acl-main.156) dataset, but the architecture is the same as in the original GPT2.

## Checkpoints
[papuGaPT2-large](https://huggingface.co/flax-community/papuGaPT2-large)